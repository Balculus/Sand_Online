---
layout: post
title: 半监督支持向量机
date: 2022-10-14
author: 来自第一世界
tags: [AI]
comments: false
---
本文主要介绍半监督支持向量机以及其中的一些问题。

# 半监督支持向量机

之所以会了解这个算法也是因为在实际中有所需求。现在有一个二分类的问题，但是问题都是无标注的数据，就算能够标注也是少数。之前的问题解决方式是通过直接设定部分阈值，直接划分。但是这样不“智能”，容易忽略很多细节，因此想采用一种分类算法来解决这个问题。

最直接也是第一个想到的算法就是 SVM，小样本下的分类算法，效果也很好。但是没有足够的数据去实现一个很好的划分，因此了解到半监督的 SVM，也就是本文的算法半监督支持向量机。

半监督支持向量机（Semi-Supervised Support Vector Machine，S3VM）是支持向量机在半监督学习上的推广。在不考虑未标记样本时，支持向量机试图找到最大间隔划分超平面，而在考虑未标记样本之后，S3VM 则考虑将两类有标记的样本分开，而且穿过数据低密度区域的超平面。

![](https://raw.githubusercontent.com/Balculus/picbed/master/2022/202302061104901.png "SVM划分")

半监督中最著名的是TSVM （Transductive Support Vector Machine）。这里需要说明一下，S3VM 和 TSVM 有相同的地方，也有不同的地方。

首先 TSVM 来源于 1999 年的文章 [TSVM](https://www-ai.cs.tu-dortmund.de/PublicPublicationFiles/joachims_99c.pdf)，阅读原始的文献可以看出与周志华《机器学习》里面的半监督 SVM 介绍是相同的。

而 S3VM 则来源于 1998 年的一篇文章 [S3VM](https://proceedings.neurips.cc/paper/1998/file/b710915795b9e9c02cf10d6d2bdb688c-Paper.pdf)，两者的形式有一定相似，但是 S3VM 还没有抽出时间细看这篇文献。但是根据从网上阅读的博客来看，S3VM 一般直接指代半监督支持向量机，因为这也是名字的直接翻译。可能一个是狭义上的，一个是广义上的。

## 算法流程

在给定数据

$$
D_l=\left \{\left(\boldsymbol{x}_1, y_1\right),    \left(\boldsymbol{x}_2, y_2\right),\ldots,\left(\boldsymbol{x}_l, y_l\right) \right \}
$$

以及未标注数据

$$
D_u=\left\{\boldsymbol{x}_{l+1},\boldsymbol{x}_{l+2},...,\boldsymbol{x}_{l+u} \}\right.
$$

的情况下，标签为 $y_i \in\{-1,+1\}$，且满足 $l \ll u, l+u=m$。

即已标注数据远小于未标注数据。那么TSVM的算法目标就是给未标注的数据一个标记

$$
\hat{\boldsymbol{y}}=(\hat{y}_{l+1}, \hat{y}_{l+2}, \ldots, \hat{y}_{l+u})
$$

使得下式满足：

$$
\min _{\boldsymbol{w}, b, \hat{\boldsymbol{y}},\boldsymbol{\xi}} \frac{1}{2}\|\boldsymbol{w}\|_2^2+C_l \sum_{i=1}^l \xi_i+C_u \sum_{i=l+1}^m \xi_i
$$

$$
\begin{array}{ll}
\text { s.t. } & y_i\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_i+b\right) \geqslant 1-\xi_i, \quad i=1,2, \ldots, l \\
& \hat{y}_i\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_i+b\right) \geqslant 1-\xi_i, \quad i=l+1, l+2, \ldots, m, \\
& \xi_i \geqslant 0, \quad i=1,2, \ldots, m,
\end{array}
$$

$(\boldsymbol{w}, b)$  实际上决定了一个超平面；$\xi$ 则为松弛变量，对应每个标记样本。$C_l$  和 $C_u$ 为学习参数，实际上体现了有标注数据和无标注数据的重要程度。

详细的算法流程如下所示：

![](https://raw.githubusercontent.com/Balculus/picbed/master/202302061026078.png)![](https://raw.githubusercontent.com/Balculus/picbed/master/2022/202302061104681.png)

这个算法的主要思想就是不断的训练出一个 SVM 模型，然后用这个模型给未标注数据进行标注，重新进行更新。

需要注意的是在选择易出错数据的时候，选择了两个标签相反，而且满足 $\xi_i+\xi_j>2$ 条件的。实际上，考虑到无标签样本，如果无标签样本在SVM模型中预测成负类或者正类，只要它与决策面的距离少于单位距离, 都会有一个损失，并且这个损失是预测成负类和预测成正类中最小的那个损失（也就是如果预测对了，这个损失可以是0）。这里所说的相加大于2，即势必有两个目前的分类结果与被打上的标签的不同，这样才能满足松弛变量大于 1。

## 代码实现

这一部分都是在网上 copy 的代码，没有运行过，放在这里主要是有个阅读代码的问题。

### 代码一

这里直接安装了一个包，算是比较容易的实现方式

```powershell
pip install semisupervised
```

```python
from semisupervised import SKTSVM

model = SKTSVM()
model.fit(np.vstack((label_X_train, unlabel_X_train)), np.append(label_y_train, 
unlabel_y))
# predict
predict = model.predict(X_test)
```

### 代码二

这一块代码是困扰我很久的一个代码。

问题在于选择易出错的数据时，选择了松弛变量最小的点，如果松弛变量最小，那么只能选择分类标记与被打的标记相同的点里，距离超平面最大的点，然后重新改变这个点的标签。这一部分因为与 TSVM 实现方法不同，所以困扰了我很久，也没有找到合理的解释。等以后有机会跑一下这个代码看看能不能正常分类。

```python
import random
import numpy as np
import sklearn.svm as svm
from sklearn.datasets.samples_generator import make_classification
from sklearn.externals import joblib
import warnings; warnings.filterwarnings(action='ignore')

class TSVM(object):
    def __init__(self, kernel='linear'):
        self.Cl, self.Cu = 1.5, 0.001
        self.kernel = kernel
        self.clf = svm.SVC(C=1.5, kernel=self.kernel)

    def train(self, X1, Y1, X2):
        N = len(X1) + len(X2)
        # 样本权值初始化
        sample_weight = np.ones(N)
        sample_weight[len(X1):] = self.Cu

        # 用已标注部分训练出一个初始SVM
        self.clf.fit(X1, Y1)
  
        # 对未标记样本进行标记
        Y2 = self.clf.predict(X2)
        Y2 = Y2.reshape(-1,1)
  
        X = np.vstack([X1, X2])
        Y = np.vstack([Y1, Y2])
  
        # 未标记样本的序号
        Y2_id = np.arange(len(X2))
  
        while self.Cu < self.Cl:
            # 重新训练SVM, 之后再寻找易出错样本不断调整
            self.clf.fit(X, Y, sample_weight=sample_weight)
            while True:
                Y2_decision = self.clf.decision_function(X2)   # 参数实例到决策超平面的距离
                Y2 = Y2.reshape(-1)
                epsilon = 1 - Y2 * Y2_decision # 松弛变量的最小值 约束中应该满足大于等于后式
                negative_max_id = Y2_id[epsilon==min(epsilon)] # 选择最小的epision即 最大的距离的未标记点 如果距离大于一 则epsilon 为0 没有损失

                print(epsilon[negative_max_id][0])
                if epsilon[negative_max_id][0] > 0:
                    # 寻找很可能错误的未标记样本，改变它的标记成其他标记
                    pool = list(set(np.unique(Y1))-set(Y2[negative_max_id]))
                    Y2[negative_max_id] = random.choice(pool) # 变成其他标记
                    Y2 = Y2.reshape(-1, 1) # 变成一列
                    Y = np.vstack([Y1, Y2])
  
                    self.clf.fit(X, Y, sample_weight=sample_weight)
                else:
                    break
            self.Cu = min(2*self.Cu, self.Cl)
            sample_weight[len(X1):] = self.Cu

    def score(self, X, Y):
        return self.clf.score(X, Y)

    def predict(self, X):
        return self.clf.predict(X)

    def save(self, path='./TSVM.model'):
        joblib.dump(self.clf, path)

    def load(self, model_path='./TSVM.model'):
        self.clf = joblib.load(model_path)

if __name__ == '__main__':
    features, labels = make_classification(n_samples=200, n_features=3, n_redundant=1, n_repeated=0, n_informative=2, n_clusters_per_class=2)
    n_given = 70
    # 取前n_given个数字作为标注集
    X1 = np.copy(features)[:n_given]
    X2 = np.copy(features)[n_given:]
    Y1 = np.array(np.copy(labels)[:n_given]).reshape(-1,1)
    Y2_labeled = np.array(np.copy(labels)[n_given:]).reshape(-1,1)

    model = TSVM()
    model.train(X1, Y1, X2)

    # Y2_hat = model.predict(X2)
    accuracy = model.score(X2, Y2_labeled)
    print(accuracy)
```

### 代码三

这一部分就是严格按照 TSVM 的实现方法，看和代码二的易错点选取就可以很清楚的看明白。

```python
# coding:utf-8
import numpy as np
import sklearn.svm as svm
from sklearn.externals import joblib
import pickle
from sklearn.model_selection import train_test_split,cross_val_score

class TSVM(object):
    def __init__(self):
        pass

    def initial(self, kernel='linear'):
        '''
        Initial TSVM
        Parameters
        ----------
        kernel: kernel of svm
        '''
        self.Cl, self.Cu = 1.5, 0.001
        self.kernel = kernel
        self.clf = svm.SVC(C=1.5, kernel=self.kernel)

    def load(self, model_path='./TSVM.model'):
        '''
        Load TSVM from model_path
        Parameters
        ----------
        model_path: model path of TSVM
                        model should be svm in sklearn and saved by sklearn.externals.joblib
        '''
        self.clf = joblib.load(model_path)

    def train(self, X1, Y1, X2):
        '''
        Train TSVM by X1, Y1, X2
        Parameters
        ----------
        X1: Input data with labels
                np.array, shape:[n1, m], n1: numbers of samples with labels, m: numbers of features
        Y1: labels of X1
                np.array, shape:[n1, ], n1: numbers of samples with labels
        X2: Input data without labels
                np.array, shape:[n2, m], n2: numbers of samples without labels, m: numbers of features
        '''
        N = len(X1) + len(X2)
        sample_weight = np.ones(N)
        sample_weight[len(X1):] = self.Cu

        self.clf.fit(X1, Y1)
        Y2 = self.clf.predict(X2)
        Y2 = np.expand_dims(Y2, 1)
        X2_id = np.arange(len(X2))
        X3 = np.vstack([X1, X2])
        Y3 = np.vstack([Y1, Y2])

        while self.Cu < self.Cl:
            self.clf.fit(X3, Y3, sample_weight=sample_weight)
            while True:
                Y2_d = self.clf.decision_function(X2)    # linear: w^Tx + b
                Y2 = Y2.reshape(-1)
                epsilon = 1 - Y2 * Y2_d   # calculate function margin
                positive_set, positive_id = epsilon[Y2 > 0], X2_id[Y2 > 0]
                negative_set, negative_id = epsilon[Y2 < 0], X2_id[Y2 < 0]
                positive_max_id = positive_id[np.argmax(positive_set)]
                negative_max_id = negative_id[np.argmax(negative_set)]
                a, b = epsilon[positive_max_id], epsilon[negative_max_id]
                if a > 0 and b > 0 and a + b > 2.0:
                    Y2[positive_max_id] = Y2[positive_max_id] * -1
                    Y2[negative_max_id] = Y2[negative_max_id] * -1
                    Y2 = np.expand_dims(Y2, 1)
                    Y3 = np.vstack([Y1, Y2])
                    self.clf.fit(X3, Y3, sample_weight=sample_weight)
                else:
                    break
            self.Cu = min(2*self.Cu, self.Cl)
            sample_weight[len(X1):] = self.Cu

    def score(self, X, Y):
        '''
        Calculate accuracy of TSVM by X, Y
        Parameters
        ----------
        X: Input data
                np.array, shape:[n, m], n: numbers of samples, m: numbers of features
        Y: labels of X
                np.array, shape:[n, ], n: numbers of samples
        Returns
        -------
        Accuracy of TSVM
                float
        '''
        return self.clf.score(X, Y)

    def predict(self, X):
        '''
        Feed X and predict Y by TSVM
        Parameters
        ----------
        X: Input data
                np.array, shape:[n, m], n: numbers of samples, m: numbers of features
        Returns
        -------
        labels of X
                np.array, shape:[n, ], n: numbers of samples
        '''
        return self.clf.predict(X)

    def save(self, path='./TSVM.model'):
        '''
        Save TSVM to model_path
        Parameters
        ----------
        model_path: model path of TSVM
                        model should be svm in sklearn
        '''
        joblib.dump(self.clf, path)

if __name__ == '__main__':
    model = TSVM()
    model.initial()
    model.train(X1, Y1, X2)
    Y_hat = model.predict(X)
    accuracy = model.score(X, Y)

```

还需要回过头来看一看。
